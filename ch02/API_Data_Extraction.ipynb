{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/c-w-m/btap/blob/master/ch02/API_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "# Chapter 02:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch02/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "if ON_COLAB:\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(BASE_DIR + '/packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust matplotlib resolution for book version\n",
    "matplotlib.rcParams.update({'figure.dpi': 200 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use APIs to extract and derive insights from text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Programming Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint - Extracting data from an API using the requests module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://api.github.com/repositories',\n",
    "                        headers={'Accept': 'application/vnd.github.v3+json'})\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding: utf-8\n",
      "Content-Type: application/json; charset=utf-8\n",
      "server: GitHub.com\n"
     ]
    }
   ],
   "source": [
    "print('encoding: {}'.format(response.encoding))\n",
    "print('Content-Type: {}'.format(response.headers['Content-Type']))\n",
    "print('server: {}'.format(response.headers['server']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Server': 'GitHub.com', 'Date': 'Tue, 20 Apr 2021 03:33:02 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Cache-Control': 'public, max-age=60, s-maxage=60', 'Vary': 'Accept, Accept-Encoding, Accept, X-Requested-With', 'ETag': 'W/\"2b44a0cea00056e0e9ee543de83bcf14d973573bb77d4f86439e9522e8f31fab\"', 'X-GitHub-Media-Type': 'github.v3; format=json', 'Link': '<https://api.github.com/repositories?since=369>; rel=\"next\", <https://api.github.com/repositories{?since}>; rel=\"first\"', 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset', 'Access-Control-Allow-Origin': '*', 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload', 'X-Frame-Options': 'deny', 'X-Content-Type-Options': 'nosniff', 'X-XSS-Protection': '0', 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'Content-Security-Policy': \"default-src 'none'\", 'Content-Encoding': 'gzip', 'X-RateLimit-Limit': '60', 'X-RateLimit-Remaining': '34', 'X-RateLimit-Reset': '1618890242', 'X-RateLimit-Used': '26', 'Accept-Ranges': 'bytes', 'Transfer-Encoding': 'chunked', 'X-GitHub-Request-Id': 'BBD8:7544:1AC5FF3:4E5C717:607E4B6D'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"node_id\": \"MDEwOlJlcG9zaXRvcnkx\",\n",
      "  \"name\": \"grit\",\n",
      "  \"full_name\": \"mojombo/grit\",\n",
      "  \"private\": false,\n",
      "  \"owner\": {\n",
      "    \"login\": \"mojombo\",\n",
      "    \"id\": 1,\n",
      "    \"node_id\": \"MDQ6VXNlcjE=\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(response.json()[0], indent=2)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/search/repositories')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/search/repositories',\n",
    "    params={'q': 'data_science+language:python'},\n",
    "    headers={'Accept': 'application/vnd.github.v3.text-match+json'})\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**data-science-from-scratch**: repository description - \"*code for Data Science From Scratch book*\" matched with **Data Science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**data-science-blogs**: repository description - \"*A curated list of data science blogs*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**data-scientist-roadmap**: repository description - \"*Toturial coming with \"data science roadmap\" graphe.*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**galaxy**: repository description - \"*Data intensive science for everyone.*\" matched with **Data**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**dsp**: repository description - \"*data science preparation*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display  ###\n",
    "def printmd(string):  ###\n",
    "    display(Markdown(string))  ###\n",
    "\n",
    "for item in response.json()['items'][:5]:\n",
    "    printmd('**' + item['name'] + '**' + ': repository ' +\n",
    "            item['text_matches'][0]['property'] + ' - \\\"*' +\n",
    "            item['text_matches'][0]['fragment'] + '*\\\" matched with ' + '**' +\n",
    "            item['text_matches'][0]['matches'][0]['text'] + '**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Code 200\n",
      "Number of comments 30\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\n",
    "print('Response Code', response.status_code)\n",
    "print('Number of comments', len(response.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': {'url': 'https://api.github.com/repositories/65600975/issues/comments?page=2',\n",
       "  'rel': 'next'},\n",
       " 'last': {'url': 'https://api.github.com/repositories/65600975/issues/comments?page=1334',\n",
       "  'rel': 'last'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages(url, params=None, headers=None):\n",
    "    output_json = []\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output_json = response.json()\n",
    "        if 'next' in response.links:\n",
    "            next_url = response.links['next']['url']\n",
    "            if next_url is not None:\n",
    "                output_json += get_all_pages(next_url, params, headers)\n",
    "    return output_json\n",
    "\n",
    "\n",
    "out = get_all_pages(\n",
    "    \"https://api.github.com/repos/pytorch/pytorch/issues/comments\",\n",
    "    params={\n",
    "        'since': '2020-07-01T10:00:01Z',\n",
    "        'sorted': 'created',\n",
    "        'direction': 'desc'\n",
    "    },\n",
    "    headers={'Accept': 'application/vnd.github.v3+json'})\n",
    "df = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "if ('body' in df.index):\n",
    "    print(df['body'].count())\n",
    "    print(df[['id','created_at','body']].sample(1, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Ratelimit-Limit 60\n",
      "X-Ratelimit-Remaining 31\n",
      "Rate Limits reset at Mon Apr 19 21:44:02 2021\n"
     ]
    }
   ],
   "source": [
    "response = requests.head(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\n",
    "print('X-Ratelimit-Limit', response.headers['X-Ratelimit-Limit'])\n",
    "print('X-Ratelimit-Remaining', response.headers['X-Ratelimit-Remaining'])\n",
    "\n",
    "# Converting UTC time to human-readable format\n",
    "import datetime\n",
    "print(\n",
    "    'Rate Limits reset at',\n",
    "    datetime.datetime.fromtimestamp(int(\n",
    "        response.headers['X-RateLimit-Reset'])).strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def handle_rate_limits(response):\n",
    "    now = datetime.now()\n",
    "    reset_time = datetime.fromtimestamp(\n",
    "        int(response.headers['X-RateLimit-Reset']))\n",
    "    remaining_requests = response.headers['X-Ratelimit-Remaining']\n",
    "    remaining_time = (reset_time - now).total_seconds()\n",
    "    intervals = remaining_time / (1.0 + int(remaining_requests))\n",
    "    print('Sleeping for', intervals)\n",
    "    time.sleep(intervals)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-science-from-scratch\n",
      "data-science-blogs\n",
      "data-scientist-roadmap\n",
      "galaxy\n",
      "dsp\n"
     ]
    }
   ],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[500, 503, 504],\n",
    "    backoff_factor=1\n",
    ")\n",
    "\n",
    "retry_adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", retry_adapter)\n",
    "http.mount(\"http://\", retry_adapter)\n",
    "\n",
    "response = http.get('https://api.github.com/search/repositories',\n",
    "                   params={'q': 'data_science+language:python'})\n",
    "\n",
    "for item in response.json()['items'][:5]:\n",
    "    print(item['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[500, 503, 504],\n",
    "    backoff_factor=1\n",
    ")\n",
    "\n",
    "retry_adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", retry_adapter)\n",
    "http.mount(\"http://\", retry_adapter)\n",
    "\n",
    "def get_all_pages(url, param=None, header=None):\n",
    "    output_json = []\n",
    "    response = http.get(url, params=param, headers=header)\n",
    "    if response.status_code == 200:\n",
    "        output_json = response.json()\n",
    "        if 'next' in response.links:\n",
    "            next_url = response.links['next']['url']\n",
    "            if (next_url is not None) and (handle_rate_limits(response)): \n",
    "                output_json += get_all_pages(next_url, param, header)\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_all_pages(\"https://api.github.com/repos/pytorch/pytorch/issues/comments\", param={'since': '2020-04-01T00:00:01Z'})\n",
    "df = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint - Extracting Twitter data with Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Host api.twitter.com\n",
      "API Version /1.1\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "app_api_key = 'YOUR_APP_KEY_HERE' \n",
    "app_api_secret_key = 'YOUR_APP_SECRET_HERE'\n",
    "\n",
    "app_api_key = 'CWIBFKPrcOU4GsdRr6J5fpaps'\n",
    "app_api_secret_key = 'SghP0LINUECDj0PzIi1vmDfRtNopqJNfb5xd3fH7XpO9ZaEtme'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(app_api_key, app_api_secret_key)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print('API Host: {}'.format(api.host))\n",
    "print('API Version: {}'.format(api.api_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Join the KOOPATROOP Discord Server! #CryptoCurrency via https://t.co/eBW8Lmmpx7 https://t.co/BnkjiAVFul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Wanchain Cryptocurrency Over 14% Down In The Last 6 Hours\\nWanchain Cryptocurrency is currently on bearish momentum… https://t.co/bz6kpYN1Ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @DannyCrypt: Now that you’ve understood what Cryptocurrency is, the only thing you’re probably thinking is what coin should I buy next?…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text\n",
       "90                                       Join the KOOPATROOP Discord Server! #CryptoCurrency via https://t.co/eBW8Lmmpx7 https://t.co/BnkjiAVFul\n",
       "53  Wanchain Cryptocurrency Over 14% Down In The Last 6 Hours\\nWanchain Cryptocurrency is currently on bearish momentum… https://t.co/bz6kpYN1Ed\n",
       "29   RT @DannyCrypt: Now that you’ve understood what Cryptocurrency is, the only thing you’re probably thinking is what coin should I buy next?…"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "search_term = 'cryptocurrency'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "                       q=search_term,\n",
    "                       lang=\"en\").items(100)\n",
    "\n",
    "retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "df = pd.json_normalize(retrieved_tweets)\n",
    "\n",
    "df[['text']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/ch02/setup.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                        count=30).items(12000)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mretrieved_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/ch02/setup.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m                        count=30).items(12000)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mretrieved_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/.tox/btap38/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/.tox/btap38/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/.tox/btap38/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/.tox/btap38/lib/python3.8/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/git.c-w-m/nlp_dev/src/btap/.tox/btap38/lib/python3.8/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note: the following code might return 'Rate limit reached. Sleeping for: 750\n",
    "api = tweepy.API(auth,\n",
    "                 wait_on_rate_limit=True,\n",
    "                 wait_on_rate_limit_notify=True,\n",
    "                 retry_count=5,\n",
    "                 retry_delay=10)\n",
    "\n",
    "search_term = 'cryptocurrency OR crypto -filter:retweets'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "                       q=search_term,\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode='extended',\n",
    "                       count=30).items(12000)\n",
    "\n",
    "retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "\n",
    "df = pd.json_normalize(retrieved_tweets)\n",
    "print('Number of retrieved tweets {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['created_at','full_text','entities.hashtags']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(entity_list):\n",
    "    entities = set()\n",
    "    if len(entity_list) != 0:\n",
    "        for item in entity_list:\n",
    "            for key,value in item.items():\n",
    "                if key == 'text':\n",
    "                    entities.add(value.lower())\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Entities'] = df['entities.hashtags'].apply(extract_entities)\n",
    "pd.Series(np.concatenate(df['Entities'])).value_counts()[:25].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "tweets = tweepy.Cursor(api.user_timeline,\n",
    "                       screen_name='MercedesAMGF1',\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode='extended',\n",
    "                       count=100).items(5000)\n",
    "\n",
    "retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "df = pd.io.json.json_normalize(retrieved_tweets)\n",
    "print('Number of retrieved tweets {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_timeline(screen_name):\n",
    "    api = tweepy.API(auth,\n",
    "                     wait_on_rate_limit=True,\n",
    "                     wait_on_rate_limit_notify=True)\n",
    "    tweets = tweepy.Cursor(api.user_timeline,\n",
    "                           screen_name=screen_name,\n",
    "                           lang=\"en\",\n",
    "                           tweet_mode='extended',\n",
    "                           count=200).items()\n",
    "    retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "    df = pd.io.json.json_normalize(retrieved_tweets)\n",
    "    df = df[~df['retweeted_status.id'].isna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mercedes = get_user_timeline('MercedesAMGF1')\n",
    "print('Number of Tweets from Mercedes {}'.format(len(df_mercedes)))\n",
    "df_ferrari = get_user_timeline('ScuderiaFerrari')\n",
    "print('Number of Tweets from Ferrari {}'.format(len(df_ferrari)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "RE_LETTER = re.compile(r'\\b\\p{L}{2,}\\b')\n",
    "\n",
    "def tokenize(text):\n",
    "    return RE_LETTER.findall(text)\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "pipeline = [str.lower, tokenize, remove_stop]\n",
    "\n",
    "def prepare(text):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "    return tokens\n",
    "\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_blueprint(df, colName, max_words, num_stopwords):\n",
    "    # Step 1: Convert input text column into tokens\n",
    "    df['tokens'] = df[colName].map(prepare)\n",
    "    \n",
    "    # Step 2: Determine the frequency of each of the tokens\n",
    "    freq_df = count_words(df)\n",
    "    \n",
    "    # Step 3: Generate the wordcloud using the frequencies controlling for stopwords\n",
    "    wordcloud(freq_df['freq'], max_words, stopwords=freq_df.head(num_stopwords).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud_blueprint(df_mercedes, 'full_text',\n",
    "                    max_words=100,\n",
    "                    num_stopwords=5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "wordcloud_blueprint(df_ferrari, 'full_text',\n",
    "                    max_words=100,\n",
    "                    num_stopwords=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "class FileStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, max_tweets=math.inf):\n",
    "        self.num_tweets = 0\n",
    "        self.TWEETS_FILE_SIZE = 10\n",
    "        self.num_files = 0\n",
    "        self.tweets = []\n",
    "        self.max_tweets = max_tweets      \n",
    "    \n",
    "    def on_data(self, data):\n",
    "        while (self.num_files * self.TWEETS_FILE_SIZE < self.max_tweets):\n",
    "            self.tweets.append(json.loads(data))\n",
    "            self.num_tweets += 1\n",
    "            if (self.num_tweets < self.TWEETS_FILE_SIZE):\n",
    "                return True\n",
    "            else:\n",
    "                filename = 'Tweets_' + str(datetime.now().time()) + '.txt'\n",
    "                print(self.TWEETS_FILE_SIZE, 'Tweets saved to', filename)\n",
    "                file = open(filename, \"w\")\n",
    "                json.dump(self.tweets, file)\n",
    "                file.close()\n",
    "                self.num_files += 1\n",
    "                self.tweets = []\n",
    "                self.num_tweets = 0\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            print('Too many requests were made, please stagger requests')\n",
    "            return False\n",
    "        else:\n",
    "            print('Error {}'.format(status_code))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_access_token = 'YOUR_USER_ACCESS_TOKEN_HERE'\n",
    "user_access_secret = 'YOUR_USER_ACCESS_SECRET_HERE'\n",
    "\n",
    "auth = tweepy.OAuthHandler(app_api_key, app_api_secret_key)\n",
    "auth.set_access_token(user_access_token, user_access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileStreamListener = FileStreamListener(20)\n",
    "fileStream = tweepy.Stream(auth=api.auth,\n",
    "                           listener=fileStreamListener,\n",
    "                           tweet_mode='extended')\n",
    "fileStream.filter(track=['cryptocurrency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open('Tweets_01:01:36.656960.txt')))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page('Cryptocurrency')\n",
    "print(p_wiki.text[:200], '....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btap38",
   "language": "python",
   "name": "btap38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
